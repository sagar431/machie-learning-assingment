{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11f618d",
   "metadata": {},
   "source": [
    "1. What is the underlying concept of Support Vector Machines?\n",
    "2. What is the concept of a support vector?\n",
    "3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a\n",
    "percentage chance?\n",
    "\n",
    "5. Should you train a model on a training set with millions of instances and hundreds of features\n",
    "using the primal or dual form of the SVM problem?\n",
    "\n",
    "6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed60983",
   "metadata": {},
   "source": [
    "Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175155dc",
   "metadata": {},
   "source": [
    "1. Support Vector Machines (SVMs) are a type of supervised machine learning algorithm used for classification and regression analysis. The underlying concept of SVMs is to find a hyperplane that best separates data points belonging to different classes by maximizing the margin between them. SVMs are particularly useful in dealing with high-dimensional data and nonlinear relationships between variables.\n",
    "\n",
    "2. A support vector is a data point that lies on the margin or within the margin of the hyperplane that separates the classes. These data points are important because they contribute to the definition of the hyperplane and the margin.\n",
    "\n",
    "3. Scaling the inputs is necessary when using SVMs because the algorithm is sensitive to the scale of the input features. Features that have a larger scale or range may dominate the algorithm, leading to inaccurate results. Scaling ensures that all features are on a comparable scale and contribute equally to the algorithm.\n",
    "\n",
    "4. When an SVM classifier classifies a case, it outputs a class label, but not a confidence score or percentage chance. The output is binary and indicates which class the input data point belongs to.\n",
    "\n",
    "5. It is better to use the dual form of the SVM problem when training a model on a training set with millions of instances and hundreds of features. The primal form becomes impractical when the number of instances is very large.\n",
    "\n",
    "6. If an SVM classifier with an RBF kernel underfits the training data, raising the value of gamma will result in a more complex model and can improve performance. However, setting gamma too high can result in overfitting. Increasing the value of C will allow more instances to be classified correctly, but can also result in overfitting.\n",
    "\n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters should be set as follows: the matrix H should be the identity matrix multiplied by the regularization parameter C, the vector f should be all -1s (for minimizing the objective function), the matrix A should be the transposed matrix of the labels (y) multiplied by the feature matrix (X), and the vector b should be all -1s (for the inequality constraint).\n",
    "\n",
    "8. It is unlikely that the models produced by LinearSVC, SVC, and SGDClassifier will be exactly the same, as they use different algorithms and have different hyperparameters. However, it is possible to achieve similar levels of performance by tuning the hyperparameters and selecting appropriate values for regularization parameters, kernel functions, and other model parameters.\n",
    "\n",
    "9. On the MNIST dataset, an SVM classifier can achieve a high level of precision (around 98% accuracy) when using an RBF kernel and optimizing hyperparameters using cross-validation. However, other classifiers such as deep neural networks have achieved even higher levels of precision on this dataset.\n",
    "\n",
    "10. On the California housing dataset, an SVM regressor can be trained to predict housing prices using various kernel functions and hyperparameters. However, other regression models such as linear regression or random forest regression may be better suited to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbef64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
